<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文精读——Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems</title>
    <url>/2020/09/05/Compositional-embedding/</url>
    <content><![CDATA[<h2 id="Compositional-Embeddings-Using-Complementary-Partitions-for-Memory-Efficient-Recommendation-Systems"><a href="#Compositional-Embeddings-Using-Complementary-Partitions-for-Memory-Efficient-Recommendation-Systems" class="headerlink" title="Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems"></a>Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems</h2>]]></content>
      <categories>
        <category>推荐系统</category>
        <category>embedding</category>
      </categories>
      <tags>
        <tag>embedding</tag>
        <tag>recommendation system</tag>
      </tags>
  </entry>
  <entry>
    <title>First Blog</title>
    <url>/2020/09/04/First-Blog/</url>
    <content><![CDATA[<h1 id="初次使用hexo搭建博客，开心，日后还要再深入"><a href="#初次使用hexo搭建博客，开心，日后还要再深入" class="headerlink" title="初次使用hexo搭建博客，开心，日后还要再深入"></a>初次使用hexo搭建博客，开心，日后还要再深入</h1>]]></content>
      <tags>
        <tag>newbie</tag>
      </tags>
  </entry>
  <entry>
    <title>HIN based Recommendation System</title>
    <url>/2020/09/05/HIN-based-Recommendation-System/</url>
    <content><![CDATA[<h2 id="HIN简介"><a href="#HIN简介" class="headerlink" title="HIN简介"></a>HIN简介</h2><ul>
<li>Metapath</li>
<li>Network schema</li>
</ul>
<h2 id="基于HIN的推荐方法"><a href="#基于HIN的推荐方法" class="headerlink" title="基于HIN的推荐方法"></a>基于HIN的推荐方法</h2><ul>
<li>在同构图上进行拓展</li>
<li>基于metapath</li>
<li>引入neighbor</li>
</ul>
]]></content>
      <categories>
        <category>Graph</category>
        <category>Recommendation System</category>
      </categories>
      <tags>
        <tag>HIN</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>MF(矩阵分解)</title>
    <url>/2020/09/05/MF/</url>
    <content><![CDATA[<h2 id="MF-矩阵分解-简介"><a href="#MF-矩阵分解-简介" class="headerlink" title="MF 矩阵分解 简介"></a>MF 矩阵分解 简介</h2><h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul>
<li>Probabilistic Metric Learning with Adaptive Margin for Top-K Recommendation</li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
        <category>矩阵分解</category>
      </categories>
      <tags>
        <tag>recommendation system</tag>
        <tag>MF</tag>
      </tags>
  </entry>
  <entry>
    <title>VScode配置remote SSH时报错：XFR failed</title>
    <url>/2020/10/01/bug0/</url>
    <content><![CDATA[<p>最近在根据知乎上的教程(<a href="https://zhuanlan.zhihu.com/p/68577071">https://zhuanlan.zhihu.com/p/68577071</a>)，希望远程SSH连接实验室服务器时报错，提示XFR failed，大概原因是vscode在remote server上所需配置下载的压缩包不能正确下载，具体解决方案有两种：</p>
<ul>
<li><p>根据报错提示，找到commit id，手动下载相应压缩包上传到server</p>
</li>
<li><p>打开 vscode remote ssh插件的setting，选择SSH.allowLocalServerDownload,也就是本地下载后自动同步到server</p>
</li>
</ul>
<p>上述操作详细步骤在此：<a href="https://stackoverflow.com/questions/56718453/using-remote-ssh-in-vscode-on-a-target-machine-that-only-allows-inbound-ssh-co/56781109#56781109">https://stackoverflow.com/questions/56718453/using-remote-ssh-in-vscode-on-a-target-machine-that-only-allows-inbound-ssh-co/56781109#56781109</a>，我尝试了第一种方法，SSH仍无法连接，而第二种方法有效。</p>
]]></content>
      <categories>
        <category>vscode</category>
      </categories>
      <tags>
        <tag>vscode</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>冷启动(cold start)问题简介及模型</title>
    <url>/2020/09/05/cold-start/</url>
    <content><![CDATA[<h2 id="冷启动-cold-start-问题简介"><a href="#冷启动-cold-start-问题简介" class="headerlink" title="冷启动(cold start)问题简介"></a>冷启动(cold start)问题简介</h2><h2 id="模型简介"><a href="#模型简介" class="headerlink" title="模型简介"></a>模型简介</h2><ul>
<li>MELU</li>
<li>MAMO</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>推荐系统</category>
        <category>冷启动</category>
      </categories>
      <tags>
        <tag>recommendation system</tag>
        <tag>cold start</tag>
      </tags>
  </entry>
  <entry>
    <title>event role</title>
    <url>/2020/09/21/event-role/</url>
    <content><![CDATA[<h2 id="事件抽取简介及doc-level-论文精读"><a href="#事件抽取简介及doc-level-论文精读" class="headerlink" title="事件抽取简介及doc-level 论文精读"></a>事件抽取简介及doc-level 论文精读</h2>]]></content>
      <categories>
        <category>nlp</category>
        <category>事件抽取</category>
      </categories>
      <tags>
        <tag>event extraction</tag>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title>Embedding在推荐系统中的应用</title>
    <url>/2020/09/22/embedding/</url>
    <content><![CDATA[<h1 id="nlp中的embedding：word2vec"><a href="#nlp中的embedding：word2vec" class="headerlink" title="nlp中的embedding：word2vec"></a>nlp中的embedding：word2vec</h1><h1 id="Rs中的embedding-：从Item2vec到阿里的embedding"><a href="#Rs中的embedding-：从Item2vec到阿里的embedding" class="headerlink" title="Rs中的embedding ：从Item2vec到阿里的embedding"></a>Rs中的embedding ：从Item2vec到阿里的embedding</h1>]]></content>
      <categories>
        <category>推荐系统</category>
        <category>Embedding</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>Recommendation System</tag>
      </tags>
  </entry>
  <entry>
    <title>first</title>
    <url>/2020/09/04/first/</url>
    <content><![CDATA[<h1 id="还没行吗"><a href="#还没行吗" class="headerlink" title="还没行吗"></a>还没行吗</h1>]]></content>
      <categories>
        <category>newbie,推荐系统</category>
      </categories>
      <tags>
        <tag>新手</tag>
      </tags>
  </entry>
  <entry>
    <title>可解释性简介</title>
    <url>/2020/09/05/graph/</url>
    <content><![CDATA[<h2 id="可解释性概念简介"><a href="#可解释性概念简介" class="headerlink" title="可解释性概念简介"></a>可解释性概念简介</h2><h2 id="关于图神经网络的可解释性"><a href="#关于图神经网络的可解释性" class="headerlink" title="关于图神经网络的可解释性"></a>关于图神经网络的可解释性</h2><ul>
<li>XGNN: Towards Model-Level Explanations of Graph Neural Networks</li>
</ul>
]]></content>
      <categories>
        <category>可解释性</category>
        <category>图神经网络</category>
      </categories>
      <tags>
        <tag>Explanation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/09/04/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Helltaker 通关感想</title>
    <url>/2020/09/20/helltaker/</url>
    <content><![CDATA[<p>这两天在网上经常看到Helltaker，又听说免费，嘿嘿嘿，下载之。<br>类似galgame，故事背景大概是主角有一天梦到了自己要在地狱开后宫，结果不知怎么真的来到了地狱，开启你的把妹之旅吧！<br>前七关都是推箱子，后面三关考验你的手速和反应，过关后会有一个恶魔加入你的后宫：每关小姐姐确实很漂亮，可惜对白太少，只能看个乐子。<br>第十关非常难，非常难，非常难，至今没过，这一关总是让我怀疑我的手指头和键盘哪一个先坏掉，后来发现是我的脑袋。<br>最终全部通关后(作者贴心的给了跳关选项)，主角带领妹子们冲出地狱，重返人间，过上了短暂又烧心的生活，这时点开ESC会发现一段方向代码，站在五角星法阵中央照着操作会开启彩蛋。</p>
]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>steam</tag>
        <tag>Helltaker</tag>
      </tags>
  </entry>
  <entry>
    <title>Memory Network在推荐系统中的应用</title>
    <url>/2020/09/30/memory/</url>
    <content><![CDATA[<h2 id="Memory-Network-在推荐系统中的应用"><a href="#Memory-Network-在推荐系统中的应用" class="headerlink" title="Memory Network 在推荐系统中的应用"></a>Memory Network 在推荐系统中的应用</h2><pre><code>    Facebook在2015年最早在paper 《Memory Network》和《End to End Memory Network》中提出了这个概念，目的是为了解决序列中的长距离依赖问题，具体模型及应用在此不多介绍，本文只引入与推荐系统相关的部分。</code></pre>
<p>​        设想中目标：Memory Network在RS中往往是作为“数据库”来使用，特别是在序列化RS中，通过自定义read和write来对用户随时间不断变化的兴趣建模。</p>
<p>​        实际上：关于memory network的相关推荐论文并不多，找了三篇简要概括如下：</p>
<p>​                在论文一号 <a href="#refer1"><sup> 1 </sup></a>以及论文二号<a href="#refer2"><sup>2</sup></a> 中，memory work仅起到“只读”效果，通过将read定义为attention操作来捕捉用户兴趣。</p>
<p>​                论文三号<a href="#refer3"><sup>3</sup></a>中，之所以引入memory，是为了</p>
<ul>
<li><p>解决序列间长距离依赖关系</p>
</li>
<li><p>使用GRU等序列化模型得到的session表示会整合session中与target item无关的item信息</p>
<p>  ​        很明显第一点提示我们要使用attention，第二点提示我们只用attention( attention 与RNN结合并不能解决长距离依赖——    Transformer )，于是作者分别在item level和feature level定义了read和write操作，read与上相同，write则采用FIFO，确保memory network仅保留最新的k个item。</p>
<p>  ​        与设想目标很接近，充分利用了memory的优点：相较于attention，我们可以通过write对信息进行改写，但在write操作上还有区别，个人觉得memory中不应保留item信息而应该是用户偏好。</p>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id="refer1"> </div>

<p>[1]  Collaborative Memory Network for Recommendation Systems, SIGIR 2018</p>
<div id="refer2"> </div>

<p>[2]A Collaborative Session-based Recommendation Approach with Parallel Memory Modules, SIGIR 2019</p>
<div id="refer3"> </div>

<p>[3]Sequential Recommendation with User Memory Networks, WSDM 2018</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>recommendation systam</tag>
        <tag>memory network</tag>
      </tags>
  </entry>
  <entry>
    <title>如何杀死nohup 运行的进程</title>
    <url>/2020/09/09/nohup/</url>
    <content><![CDATA[<p>在服务器上跑程序的时候为了后台运行，使用了nohup命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nohup python main.py</span><br></pre></td></tr></table></figure>
<p>当结束进程时，用<code>ps -u</code>查找到了<code>python main.py</code>这个进程的ID，直接kill发现进程没有被杀死。于是 使用<code>ps -ef|grep nohup</code>找到<code>nohup</code>的进程ID， <code>kill -9 ID</code>干掉之后再重新<code>kill</code>刚才的<code>python main.py</code>进程，发现已经被杀死。(偶然发现，仅供参考)</p>
]]></content>
      <categories>
        <category>-linux指令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nohup</tag>
      </tags>
  </entry>
  <entry>
    <title>K-NRM与Conv K-NRM简介</title>
    <url>/2020/12/07/k-nrm/</url>
    <content><![CDATA[<ul>
<li><p>K-NRM（SIGIR 2017）简介</p>
<ul>
<li><p>论文要解决的问题/目标：</p>
<ul>
<li>抽取并利用多粒度/多尺度 soft-match feature</li>
</ul>
</li>
<li><p>可能的指导原则(我觉得:)——语义相似度与相关性并不等价</p>
<ul>
<li>这里举两个例子，苹果与库克相关性很强，但语义上相差比较大；波士顿和纽约相似度高，但不相关。第二个例子要注意一个问题，语义相似度是由什么定义/计算的？如果是人为判断，那第二个例子不成立，如果是指word2vec词向量做内积，那第二个例子成立，因为word2vec相当于在做分类，现在2020年，若采用BERT等做相似度表示，语义和相关性的关系是否发生了变化？</li>
</ul>
</li>
<li><p>什么是soft-match？</p>
<ul>
<li>先介绍exact-match：query 与doc完全一致，此时cosine(q,d)=1</li>
<li>相对于exact-match，soft-match指α&lt;cosine(q,d)&lt;1，换句话说，语义相似度较高的(query,doc)</li>
</ul>
</li>
<li><p>本文思路（怎么利用soft-match）：</p>
<ul>
<li><p>给定query和doc，以词为单位得到各词的word2vec embedding，计算词与词之间的相似度，称作translation matrix，再对这个matrix中的每一行使用不同的RBF抽取$ q_i$  在不同相似度下关于doc的分布(kernal pooling)，基于分布情况计算最终的q,d的相关性,模型实质上是通过不断调整embedding来提高q,d相关性，其架构如下图所示：</p>
</li>
<li><p><img src="https://s3.ax1x.com/2020/12/07/Dzu6l4.png" title="K-NRM模型图示"></p>
</li>
</ul>
</li>
<li><p>kernal pooling</p>
<ul>
<li>这部分是整个模型的核心，直接上公式：</li>
<li><img src="https://s3.ax1x.com/2020/12/07/DzMgd1.png" alt="avatar"></li>
<li><img src="https://s3.ax1x.com/2020/12/07/DzMfJK.png" alt="avatar"></li>
</ul>
</li>
<li><p>前两行公式是指对M中的每一行，也就是query中的每个word用不同的RBF$ K_k $ 做pooling，再加到一起，可以看作query对于doc中每个词在不同similarity level下的分布</p>
<ul>
<li>第三行公式RBF可以看作对于pooling的泛化，当σ趋于无穷，RBF退化到mean pooling，当σ趋于0，μ=1，kernel的值只在$ M_{ij}$ 等于1时有意义，即 exact match，也可以说是TF，所以这个kernel 也叫 soft-TF。</li>
</ul>
</li>
<li><p>模型公式汇总:</p>
<ul>
<li><img src="https://s3.ax1x.com/2020/12/07/Dztkxx.png" alt="avatar"></li>
<li>最后线性层中的w中的正负可以看作(query_word,doc_word)pair对于相关性的影响</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>​    </p>
<ul>
<li><p>Conv K-NRM简介</p>
<ul>
<li><p>在上篇论文的基础上引入了n-gram，在interaction layer做了改进</p>
<ul>
<li>原因：进行不同粒度间的交互，如apple 与Stephen Jobs</li>
</ul>
</li>
<li><p>不能直接使用n-gram的embedding：</p>
<ul>
<li><p>原因：以词为单位的n-gram数量巨大，直接作embedding参数太多</p>
</li>
<li><p>解决：使用CNN 不同尺寸的卷积核构建不同长度的 n-gram</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>信息检索</category>
      </categories>
      <tags>
        <tag>information retrieval</tag>
        <tag>normal distribution</tag>
        <tag>RBF</tag>
      </tags>
  </entry>
  <entry>
    <title>基于FM的Recommendation</title>
    <url>/2020/09/05/%E5%9F%BA%E4%BA%8EFM%E7%9A%84Recommendation/</url>
    <content><![CDATA[<h2 id="FM-Factorization-Machine-分解机-简介"><a href="#FM-Factorization-Machine-分解机-简介" class="headerlink" title="FM(Factorization Machine)分解机 简介"></a>FM(Factorization Machine)分解机 简介</h2><h2 id="与深度学习相结合"><a href="#与深度学习相结合" class="headerlink" title="与深度学习相结合"></a>与深度学习相结合</h2><ul>
<li>DeepFM</li>
<li>AutoFM</li>
<li>GBDT</li>
</ul>
]]></content>
      <tags>
        <tag>-推荐系统 -FM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文精读——CAST: A Correlation-based Adaptive Spectral Clustering Algorithm on Multi-scale Data</title>
    <url>/2020/09/05/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94ST-A-Correlation-based-Adaptive-Spectral-Clustering-Algorithm-on-Multi-scale-Data/</url>
    <content><![CDATA[<h2 id="CAST-A-Correlation-based-Adaptive-Spectral-Clustering-Algorithm-on-Multi-scale-Data"><a href="#CAST-A-Correlation-based-Adaptive-Spectral-Clustering-Algorithm-on-Multi-scale-Data" class="headerlink" title="CAST: A Correlation-based Adaptive Spectral Clustering Algorithm on Multi-scale Data"></a>CAST: A Correlation-based Adaptive Spectral Clustering Algorithm on Multi-scale Data</h2>]]></content>
      <tags>
        <tag>Spectral Clustering</tag>
      </tags>
  </entry>
  <entry>
    <title>Stance Detection(立场检测)简介</title>
    <url>/2020/09/06/stance-detection/</url>
    <content><![CDATA[<h2 id="Stance-Detection-立场检测-简介"><a href="#Stance-Detection-立场检测-简介" class="headerlink" title="Stance Detection 立场检测 简介"></a>Stance Detection 立场检测 简介</h2><h2 id="与新闻推荐相结合"><a href="#与新闻推荐相结合" class="headerlink" title="与新闻推荐相结合"></a>与新闻推荐相结合</h2><blockquote>
<p>将新闻中的实体抽取出来（篇章或题目），判断支持、反对、中立，根据阅读历史记录，向用户推荐相同立场倾向的新闻</p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
        <category>情感分析</category>
        <category>立场检测</category>
      </categories>
      <tags>
        <tag>Stance Detection</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
